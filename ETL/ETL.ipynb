{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCION PARRA ARREGLAR LOS JSON CORRUPTOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "## ETL STEAM_GAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos y creamos el dataframe steam_games\n",
    "steam_games = pd.read_parquet(\"../Datasets/steam_games_arreglado.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borramos filas nulas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borramos columnas que no vamos a usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borramos las columnas inecesarias\n",
    "columns_delete_games = [\"publisher\", \"title\", \"url\", \"tags\", \"reviews_url\", \"specs\", \"early_access\"]\n",
    "steam_games.drop(columns=columns_delete_games, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renombramos las columnas para asemejar con las otras base de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificamos los nombres de la columna id = item_id, app_name = item_name para futuros merge con los otros datasets\n",
    "steam_games.rename(columns = {\"id\": \"item_id\",\"app_name\" : \"item_name\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desanidamos la columna genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games = steam_games.explode(column='genres')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos la columna release_date para quedarnos con el año solo y llenamos los valores nulos con Unknown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecemos la columna year paara los años y la convertimos en int y los nulos en 0 \n",
    "steam_games[\"release_date\"] = pd.to_datetime(steam_games['release_date'], errors='coerce') #Convertimos en la columna release_date a valores date\n",
    "\n",
    "steam_games['release_year'] = steam_games['release_date'].dt.year.fillna(-1).astype(int) #  ceamos la columna release_year con los años de la columna release_date y los nulos son 'Unknown'\n",
    " \n",
    "steam_games['release_year'] = steam_games['release_year'].astype(str).replace('-1','Unknown') # Comvertimos finalmente en string y las fechas que eran nulas en Unknown(Desconocido)\n",
    "\n",
    "# Borramos la columna vieja 'release_date'\n",
    "steam_games.drop(columns='release_date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos los valores en entero los valores de la columna 'item_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games[\"item_id\"] = steam_games[\"item_id\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos en float los valores de la columna  price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games['price'] = steam_games['price'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borramos duplicados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reseteamos el indice y ordenamos segun la columna item_id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games.sort_values(by=\"item_id\", inplace=True)\n",
    "\n",
    "steam_games.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos dataset final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_games.to_parquet(\"../Datasets_final/steam_games_final.parquet.gz\", compression = \"gzip\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "## TRABAJAMOS CON  ARCHIVO USER_REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reviews = pd.read_parquet('../Datasets/user_reviews_arreglado.parquet') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desanidamos la columna 'reviews':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicados = user_reviews.explode(column= \"reviews\") # creamos un dataframe donde duplicamos las filas de acuerdo a la cantidad de subcampos que hay en la columna review.    \n",
    "\n",
    "# BORRAMOS LAS FILAS NULAS QUE TENGA REVIEW\n",
    "df_duplicados.dropna(subset=\"reviews\", inplace = True) # Borramos las filas de la columna reviews que sean nulos, ya que es la unica columna que tiene nulos\n",
    "\n",
    "# RESETEAMOS INDICE PARA CONCATENAR DESPUES\n",
    "df_duplicados.reset_index(inplace=True, drop=True) # Reseteamos los indices del dataframe desanidado para poder concatenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREAMOS UN DADAFRAME CON LA COLUMNA REVIEWS YA DESANIDADA.\n",
    "\n",
    "diccionario = { \"funny\" : [],  # Creamos diccionario donde se almacenan los datos de la columna reviews\n",
    "                \"posted\" : [],\n",
    "                \"last_edited\" : [],\n",
    "                \"item_id\" : [],\n",
    "                \"helpful\" : [],\n",
    "                \"recommend\": [],\n",
    "                \"review\" : []}\n",
    "\n",
    "for fila in df_duplicados[\"reviews\"]:     # Ieramos la columna para que me queden solo los diccionarios\n",
    "    for i, items in enumerate(fila.values()):  #Iteramos cada valor del diccionario para que se agregue al diccionario antes creado\n",
    "        if i == 0:\n",
    "                diccionario[\"funny\"].append(items)\n",
    "        elif i == 4:\n",
    "                diccionario[\"posted\"].append(items)\n",
    "        elif i ==2:\n",
    "                diccionario[\"item_id\"].append(items)\n",
    "        elif i == 3:\n",
    "                diccionario[\"last_edited\"].append(items)\n",
    "        elif i == 1:\n",
    "                diccionario[\"helpful\"].append(items)\n",
    "        elif i == 5:\n",
    "                diccionario[\"recommend\"].append(items)\n",
    "        elif i == 6:\n",
    "                diccionario[\"review\"].append(items)\n",
    "\n",
    "df_separado_reviews = pd.DataFrame(diccionario) # Creamos el df de solo los elementos de la columna reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unimos los dos dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenamos en un solo dataframe\n",
    "user_reviews_final = pd.concat([df_duplicados,df_separado_reviews],axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGREGAR COLUMNA DE ANALISIS DE SENTIMIENTO AL DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_analysis\n",
       "2    26263\n",
       "1    24959\n",
       "0     8083\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREAMOS LA FUNCION PARA REALIZAR UN ANÁLISIS DE SENTIMIENTO\n",
    "from textblob import TextBlob # Usamos la libreria textblob para ello\n",
    "\n",
    "def analisis_de_sentimiento(cadena_caracteres):\n",
    "    \"\"\"Esta funcion devuelve un valor segun el analisis de sentimiento que realize al strig. Como parametro recibe un str y puede devolver (0: Malo, 1: Neutral, 2: Positivo )\"\"\"\n",
    "    \n",
    "    analizador = TextBlob(str(cadena_caracteres)) # La libreria Texplot permite hacer un analisis de sentimiento sobre un string\n",
    "    \n",
    "    if analizador.sentiment.polarity > 0.1: \n",
    "        return 2  \n",
    "    elif analizador.sentiment.polarity < -0.1:\n",
    "        return 0  \n",
    "    else:\n",
    "        return 1 \n",
    "    \n",
    "\n",
    "# Aplicamos la funcion a la columna review\n",
    "user_reviews_final[\"sentiment_analysis\"] = user_reviews_final[\"review\"].apply(analisis_de_sentimiento) \n",
    "\n",
    "# Comprobamos el analisis de sentimiento\n",
    "user_reviews_final.sentiment_analysis.value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borramos columnas innecesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BORRAMOS LAS COLUMNAS QUE NO VAMOS A NECESITAR:\n",
    "columnas_borrar_reviews = [\"user_url\", \"funny\", \"posted\", \"last_edited\", \"helpful\", \"reviews\", \"review\"] # Establecemos las columnas a borrar\n",
    "user_reviews_final.drop(columns=columnas_borrar_reviews, inplace=True, axis=1) # Borramos esas columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos la columna item_id a entero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna item_id a tipo entero\n",
    "user_reviews_final['item_id'] = user_reviews_final['item_id'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borramos duplicados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reviews_final.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenamos y reseteamos indice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reviews_final.sort_values(by=\"item_id\", inplace=True) #Ordenamos por item_id\n",
    "\n",
    "user_reviews_final.reset_index(inplace=True, drop=True) # Reseteamos index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datasets final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reviews_final.to_parquet(\"../Datasets_final/user_reviews_final.parquet.gz\", compression = \"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "## TRABAJAMOS CON  ARCHIVO USER_ITEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items = pd.read_parquet('../Datasets/user_items_arreglado.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desanidamos columna items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicados2 = user_items.explode(column= \"items\") # creamos dataframe donde duplica los campos de los usuarios de acuerdo a la cantidad de campos de reviews o items que haya en esa calumna por usuario .  \n",
    "\n",
    "df_duplicados2.dropna(subset='items', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREAMOS UN DATAFRAME LA COLUMNA DF DESANIDADA\n",
    "\n",
    "diccionario2 = { \"item_id\" : [],  # Creamos diccionario donde se almacenan los datos que estan en los diccionarios\n",
    "                \"item_name\" : [],\n",
    "                \"playtime_forever\" : [],\n",
    "                \"playtime_2weeks\" : []}\n",
    "\n",
    "for fila in df_duplicados2[\"items\"]:     # Ieramos la columna para que me queden solo los diccionarios\n",
    "    for i, items in enumerate(fila.values()):  #Iteramos cada valor del diccionario para que se agregue al diccionario antes creado\n",
    "        if i == 0:\n",
    "                diccionario2[\"item_id\"].append(items)\n",
    "        elif i == 1:\n",
    "                diccionario2[\"item_name\"].append(items)\n",
    "        elif i ==3:\n",
    "                diccionario2[\"playtime_forever\"].append(items)\n",
    "        elif i == 2:\n",
    "                diccionario2[\"playtime_2weeks\"].append(items)\n",
    "\n",
    "\n",
    "df_separados_items = pd.DataFrame(diccionario2) # Creamos el df de la columna items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseteamos el indice para concatenar\n",
    "df_duplicados2.reset_index(inplace=True, drop=True) # Reseteamos ya que de lo contrario no nos dejara concatenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenamos\n",
    "user_items_final = pd.concat([df_duplicados2,df_separados_items],axis=1) # Concatenamos los dos dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borramos columnas que no necesitamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borramos columnas que no usemos\n",
    "columnas_borrar_items = [\"steam_id\",\"items\", \"user_url\",\"items\",\"playtime_2weeks\"]\n",
    "user_items_final.drop(columns=columnas_borrar_items, inplace = True) # Borramos las columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos item_id en INT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMAMOS LA COLUMNA item_id\n",
    "\n",
    "user_items_final[\"item_id\"] = user_items_final[\"item_id\"].astype(int) # Convertimos en int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borramos duplicados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items_final.drop_duplicates(inplace=True) # Borramos filas duplicadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenamos y reseteamos indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items_final.sort_values(by=\"item_id\",inplace=True) # Ordenamos por item_id\n",
    "\n",
    "user_items_final.reset_index(inplace=True, drop=True) # Reseteamos index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datasets final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items_final.to_parquet(\"../Datasets_final/user_items_final.parquet.gz\",index=False, compression = \"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
